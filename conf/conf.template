[HDFS]
# hdfs namenode host
namenode: hdfs://hdfs_test_host:hdfs_test_host
# hdfs user
user: hdfs_test_user
# hdfs rollback days:
rollback_days: 3
# hdfs path for metric data
path_metric: {{namenode}}/user/{{hdfs_user}}/argo/tenants/{{tenant}}/mdata
# hdfs path for sync data
path_sync: {{namenode}}/user/{{hdfs_user}}/argo/tenants/{{tenant}}/sync

# hdfs writer executable
writer_bin: /path/to/binary

[API]
endpoint = api.foo
tenants = TENANTA
access_token = key0
TENANTA_key = key1
TENANTB_key = key2
TENANTC_key = key3


[LOGS]
# log handlers to support
log_modes: console,syslog,file
# global log level
log_level: info
# specific level for syslog handler
syslog_level: critical
# specific level for file handler
file_level: warning
# specific level for console handler
console_level: info
# socket for syslog handler
syslog_socket: /dev/log
# path for file handler
file_path:

[FLINK]
# path for flink executable
path: flink_path
# url for flink job manager
job_manager:

[JOB-NAMESPACE]
# Template to check if a metric job with similar name already runs
ingest-metric-namespace: Ingesting data from {{ams_endpoint}}:{{ams_port}}/v1/projects/{{ams_project}}/subscriptions/{{ams_sub}}
# Template to check if a sync job with similar name already runs
ingest-sync-namespace: Ingesting sync data from {{ams_endpoint}}:{{ams_port}}/v1/projects/{{ams_project}}/subscriptions/{{ams_sub}}
#Template to check if a stream status job with similar name already runs
stream-status-namespace: Streaming status using data {{ams_endpoint}}:{{ams_port}}/v1/projects/{{ams_project}}/subscriptions/[{{ams_sub_metric}}, {{ams_sub_sync}}]

[CLASSES]
# Specify class to run during job submit
ams-ingest-metric: test_class
ams-ingest-sync: test_class
batch-ar: test_class
batch-status: test_class
stream-status: test_class

[JARS]
# Specify jar to run during job submit
ams-ingest-metric: test.jar
ams-ingest-sync: test.jar
batch-ar: test.jar
batch-status: test.jar
stream-status: test.jar


[AMS]
endpoint: https://test_endpoint:8080
# Ams proxy
proxy:test_proxy
# Verify ssl
verify: true

# Tenant General Configuration
[TENANTS:TENANTA]
# Map tenant to AMS project
ams_project: test_project
# Tenant's AMS token
ams_token: test_token
reports: report_name
report_report_name: report_uuid
mongo_uri: mongodb://mongo_test_host:21017/argo_TENANTA


# Tenant-job specific configuration
[TENANTS:TENANTA:ingest-metric]
# AMS subscription for ingesting data
ams_sub: job_name
# Interval between each ingestion request
ams_interval: 300
# Max number of messages to ingest each time
ams_batch: 100
# Path to save flink checkpoint
checkpoint_path:test_path
# Interval between flink checkpoints
checkpoint_interval: 30000
[TENANTS:TENANTA:ingest-sync]
ams_sub: job_name
ams_interval: 3000
ams_batch: 100
[TENANTS:TENANTA:stream-status]
# whether or not it should also include mongo information(it will access the section TENANTS:TENANTA:MONGO)
mongo_method:upsert
ams_batch: 10
ams_interval: 300
# ARGO messaging subscription to pull metric data from
ams_sub_metric: metric_status
# ARGO messaging subscription to pull sync data from
ams_sub_sync: sync_status
#hbase endpoint
hbase_master: hbase://hbase.devel:8080
# comma separated list of hbase zookeeper servers
hbase_zk_quorum: test_zk_servers
# port used by hbase zookeeper servers
hbase_zk_port: 8080
# table namespace used (usually tenant name)
hbase_namespace: test_hbase_namespace
# table name (usually metric_data)
hbase_table: metric_data
# Kafka server list to connect to
kafka_servers: kafka_server:9090,kafka_server2:9092
# Kafka topic to send status events to
kafka_topic: test_kafka_topic
# filesystem path for output (prefix with "hfds://" for hdfs usage)
fs_output: test_fs_output
# flink parallelism for specific tenant and job(execution environment level)
flink_parallelism: 1
# different output destinations for the results
outputs: hbase,kafka,fs,mongo
